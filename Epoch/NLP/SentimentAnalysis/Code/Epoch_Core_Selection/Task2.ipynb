{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Documentation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideas Involved:\n",
    "1. Removing Stop words but keeping negation word as they hold importance for analysing the sentiments\n",
    "2. Removing Proper Nouns (Reduce processing time)\n",
    "3. Using frequencies of words generated over whole dataset as a metric to convert reviews into vector (More detail in Method 1)\n",
    "4. For a particular word in a word vector list, using its similar words in a word dataset (which can have low frequency but hold sentiments) to convert reviews into vector (More detail in Method 2)\n",
    "\n",
    "### **Analysing Performance of Different Classifier**\n",
    "| Classifier |Method 1 $\\\\$(500 features) $\\\\$ (Without Removing Nouns) |Method 1 $\\\\$(500 features) $\\\\$ (With Removing Nouns)|Method 1 $\\\\$(1000 features) $\\\\$ (With Removing Nouns)| Method 2 $\\\\$ (1000 features) $\\\\$ (Without Removing Noun) | Method 2 $\\\\$(1000 features)$\\\\$ (With Removing Noun)\n",
    "|:-----------|:-------------------:|:-------------------:|:-------:|:-----:|:-----:|\n",
    "|Logistic Regression|**0.8355** (2.7 sec)|**0.8398** (2.3 sec) |**0.84288** (5.2 sec)| **0.8576** | **0.8628**|\n",
    "|Naive Bayes (LDA)|**0.8296** (5.4 sec)|**0.8336** (3.7 sec)|**0.83488** (8 sec) | **0.85296** | **0.8588** |\n",
    "|KNN (k=47)|**0.7206** (15 sec) |**0.7184** (7 sec)|**0.71088** (8.3 sec) | NA | NA|\n",
    "|SVM|**0.8301** (11 min)|**0.8349** (7 min 13 sec)|**0.8384** (15 min) | NA | NA |\n",
    "|Random Forest(1000 DT)|**0.7835** (28 min) | NA |NA | NA | NA|\n",
    "\n",
    "**Note:** \n",
    "1. **NA** means accuracy is not calculated due to high processing time and availability of better classifier. \n",
    "2. For KNN, k=47 is choosen after iterating over multiple k values and finding the one with max accuracy. \n",
    "3. For Random Forest, increasing DT increase the accuracy but also increase the processing time.\n",
    "4. Max accuracy over Validation Dataset achieved is **0.8628**\n",
    "\n",
    "### Improvement:\n",
    "1. Removing proper nouns helps in increasing accuracy\n",
    "2. Adding Negation words increase the accuracy from **0.8576** to **0.8628** (in Method 2 with removing nouns)\n",
    "\n",
    "### Kaggle Score\n",
    "1. **Score :0.85332**\n",
    "\n",
    "Here is the reference\n",
    "![image info](kaggle_score.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install nltk gensim textblob scikit-learn scikit-image pandas numpy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python Inbuilt Libraries\n",
    "import logging      # Displaying Formatted Log Output\n",
    "import re           # Regular Expression\n",
    "import string       # Ueed to get punctuations (string.punctuations)\n",
    "import operator     # Number of Occurrence of Word in a List\n",
    "import collections  # Generating Frequecy of words in a list\n",
    "import os           # Location and Finding Files\n",
    "\n",
    "# Data Processing Libraries\n",
    "import pandas as pd # Processing Input and Output Files (.tsv)\n",
    "import numpy as np  # Handling Arrays\n",
    "\n",
    "# NLP libraries\n",
    "import nltk.data                                 # Tokenization\n",
    "from nltk.corpus import stopwords           # Finding StopWords\n",
    "from nltk.stem import \tWordNetLemmatizer   # Lemmatization\n",
    "from textblob import TextBlob               # POS tagging\n",
    "from gensim.models import word2vec          # Word2Vector\n",
    "\n",
    "# Classification Model Libraries\n",
    "from sklearn.model_selection import train_test_split    # Splitting training Dataset\n",
    "from sklearn.ensemble import RandomForestRegressor      # Random Forest\n",
    "from sklearn import svm                                 # SVM\n",
    "from sklearn.neighbors import KNeighborsClassifier      # KNN\n",
    "from sklearn.naive_bayes import GaussianNB              # Gaussion Distribution (Naive Bayes)\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis # LDA,QDA (Naive Bayes)\n",
    "from sklearn.linear_model import LogisticRegression     # Logistic Regression \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Data (Labeled)\n",
    "labelled_train = pd.read_csv( \"../Dataset/labeledTrainData.tsv\", header=0, delimiter=\"\\t\", quoting=3 )\n",
    "# Training Data (Unlabeled)\n",
    "unlabeled_train = pd.read_csv( \"../Dataset/unlabeledTrainData.tsv\", header=0, delimiter=\"\\t\", quoting=3 )\n",
    "# Test Data\n",
    "test = pd.read_csv( \"../Dataset/testData.tsv\", header=0, delimiter=\"\\t\", quoting=3 )\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean Reviews\n",
    "Cleaning Involves\n",
    "1. Remove HTML Tags\n",
    "2. Remove Non-alphabetic Symbols\n",
    "3. Lemmatization, Removing Proper Nouns, Remove Stop Words **(Keeping Negation Stop Words)**, Remove Punctuations and Empty String\n",
    "4. Storing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Intialize Lemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "CLEANR = re.compile('<.*?>') \n",
    "# Negation Words (to be removed from stop words)\n",
    "negation_words = [\"not\",\"nor\",\"none\",\"never\",\"neither\"]\n",
    "\n",
    "def clean_review(revs_unclean,removeStopWords = False,removeNoun = False, display = False,needFlattenWords = False):\n",
    "    '''\n",
    "    @param:\n",
    "        revs_unclean:       list of strings (list of uncleaned reviews string)\n",
    "        removeStopWords:    bool (marker for removing stop words)\n",
    "        removeNoun:         bool (marker for removing Proper Nouns)\n",
    "        display:            bool (marker for displaying info)\n",
    "        needFlattenWords:   bool (marker for flattening the list of list of words)\n",
    "\n",
    "    @output:\n",
    "        rev_clean: list of list of string (list of list containing cleaned words of the reviews)\n",
    "        all_words: flattened rev_clean (used to get word frequency)\n",
    "    '''\n",
    "\n",
    "    # Stop words exculding negation words\n",
    "    stops = set(stopwords.words(\"english\")) - set(negation_words)\n",
    "\n",
    "    # checker for Noun (Proper Noun)\n",
    "    is_noun = lambda pos: pos[:2] == 'NN' and len(pos) >2\n",
    "\n",
    "    rev_clean =[] #Intialize empty clean review array\n",
    "\n",
    "    all_words = []\n",
    "\n",
    "    if display:\n",
    "        print(\"Cleaning Started\")\n",
    "        \n",
    "    cleaned = 0\n",
    "    for review in revs_unclean:\n",
    "        \n",
    "        # if review is empty\n",
    "        if not (len(review) > 0):\n",
    "            print(\"\\nEmpty review found\")\n",
    "            continue\n",
    "        \n",
    "        #otherwise\n",
    "\n",
    "        # 1. Remove HTML Tags\n",
    "        review_text = re.sub(CLEANR, '', review)\n",
    "\n",
    "        # 2. Remove Non-alphabetic Symbols\n",
    "        review_text = re.sub(\"[^a-zA-Z]\",\" \",review_text)\n",
    "\n",
    "        if removeNoun:\n",
    "            # POS Tagging\n",
    "            blob = TextBlob(review_text)\n",
    "        \n",
    "            # 3. Lemmatization, Removing Proper Nouns, Remove Stop Words (Keeping Negation Stop Words), Remove Punctuations and Empty String\n",
    "            words = [\n",
    "                wordnet_lemmatizer.lemmatize(w.lower())                     # Storing Lemmatized word\n",
    "                for w,pos in blob.pos_tags \n",
    "                    if (not is_noun(pos))                                   # Removing Nouns\n",
    "                    and             \n",
    "                    (not removeStopWords or w.lower() not in stops)         # Removing Stop Words\n",
    "                    and    \n",
    "                    w not in string.punctuation                             # Removing Punctuations\n",
    "                    and\n",
    "                    len(w) > 0                                              # Non Empty\n",
    "                ]\n",
    "        else:\n",
    "\n",
    "            # 3. Lemmatization, Remove Stop Words (Keeping Negation Stop Words), Remove Punctuations and Empty String\n",
    "            words = [\n",
    "                wordnet_lemmatizer.lemmatize(w.lower())  \n",
    "                for w in review_text.split()\n",
    "                if (not removeStopWords or w.lower() not in stops)         # Removing Stop Words\n",
    "                    and    \n",
    "                    w not in string.punctuation                            # Removing Punctuations\n",
    "                    and\n",
    "                    len(w) > 0                                             # Non Empty\n",
    "            ]\n",
    "        \n",
    "        # 4. Storing \n",
    "        rev_clean.append(words)\n",
    "\n",
    "        # Flattening the words\n",
    "        if needFlattenWords:\n",
    "            all_words += words\n",
    "\n",
    "        # Display\n",
    "        if display:\n",
    "            cleaned+= 1\n",
    "            print(\"\\f\\rCleaned: {}/{}\".format(cleaned,len(revs_unclean)),end=\"\")\n",
    "    if display:\n",
    "        print(\"\\nReviews Cleaned!\")\n",
    "    return rev_clean,all_words\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding Most Frequent Words\n",
    "To create vectors from reviews (list of words), we need most frequent words used in every reviews (Excluding Stop words) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def word_frequency_in_descending_order(all_words):\n",
    "    '''\n",
    "    @param\n",
    "        all_words: list of strings (list of  words)\n",
    "    @output\n",
    "        frequent_words: list of words sorted by their frequency in descending order (most frequent is at first)\n",
    "    '''\n",
    "\n",
    "    # count the word frequency and create dict\n",
    "    word_freq = dict(collections.Counter(all_words))\n",
    "\n",
    "    # sort the dict by their values (frequency)\n",
    "    frequent_words = sorted(word_freq,key=word_freq.get,reverse=True)\n",
    "\n",
    "    return frequent_words"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method 1\n",
    "This Method Invloves directly convert the reviews (list of words) into vector (list of numbers) by counting the occurrence of a word from a word_list in the review.\n",
    "Vector[i] = number of occurrence of Word_List[i] in reviews[i]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Vector from review "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_vector_direct(revs_clean,words):\n",
    "    '''\n",
    "    @param:\n",
    "        revs_clean: list of list of string (list of cleaned reviews (list of words) )\n",
    "        words: list of words (word list vector)\n",
    "    @output;\n",
    "        list of list of numbers (list of vectors representing reviews)\n",
    "    '''\n",
    "    print(\"Generating Vectors\")\n",
    "    return [\n",
    "            [\n",
    "                operator.countOf(review,word) # counting occurrence of word in the review\n",
    "                for word in words\n",
    "            ]\n",
    "            for review in revs_clean\n",
    "        ]\n",
    " "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method 2\n",
    "This method involves creating a word_matrix instead of word_list vector. Word_list matrix contains most frequent words (similar to word_list vector) as their first column whereas other columns do contain the word similar (by meaning) to the first word of their respective row.\n",
    "```\n",
    "word_matrix: [\n",
    "    most freq word 1 (W1) , words with similar meaning to W1 ...\n",
    "    most freq word 2 (W2) , words with similar meaning to W2 ...\n",
    "    most freq word 3 (W3) , words with similar meaning to W3 ...\n",
    "    most freq word 4 (W4) , words with similar meaning to W4 ...\n",
    "    .\n",
    "    .\n",
    "    .\n",
    "]\n",
    "```\n",
    "\n",
    "Now, Vector[i] =  $\\sum_{w \\in word\\_matrix[i]} $ Occurrence of w in review[i]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Dataset for the word2vec\n",
    "To get similar words, we will use word2vec to get the relationship between words. We will use reviews from labelled and unlabelled dataset to prepare dataset for word2vec "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate list of list of words from review\n",
    "def get_sentences(review,tokenizer):\n",
    "\n",
    "    '''\n",
    "    @param:\n",
    "        review: string (uncleaned review)\n",
    "        tokenizer: tokenizer \n",
    "    @output:\n",
    "        list of list of processed words\n",
    "    '''\n",
    "\n",
    "    # Tokenization\n",
    "    raw_sentences = tokenizer.tokenize(review.strip())\n",
    "\n",
    "    # returning cleaned reviews\n",
    "    return clean_review(raw_sentences)[0]\n",
    "\n",
    "def prepare_dataset():\n",
    "    \n",
    "    '''\n",
    "    @param:\n",
    "        None\n",
    "    @output:\n",
    "        sentences: list of list of strings (list of list of processed words)\n",
    "    '''\n",
    "    # Intialize tokenizer\n",
    "    tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    sentences = []  \n",
    "\n",
    "    # Parsing labeled_train data\n",
    "    print( \"Parsing sentences from training set\")\n",
    "    for review in labelled_train[\"review\"]:\n",
    "        sentences += get_sentences(review, tokenizer)\n",
    "\n",
    "    # Parsing unlabeled_train data\n",
    "    print( \"Parsing sentences from unlabeled set\")\n",
    "    for review in unlabeled_train[\"review\"]:\n",
    "        sentences += get_sentences(review, tokenizer)\n",
    "    \n",
    "    return sentences\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Word2Vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_model(sentences,num_features,min_word_count,num_workers,context,downsampling,epoch = 5,PrintLogInfo = True):\n",
    "    '''\n",
    "    @param:\n",
    "        sentences:  list of list of words (dataset)\n",
    "        num_features:       Word vector dimensionality  \n",
    "        min_word_count:     Minimum word count\n",
    "        num_workers:        Number of threads to run in parallel\n",
    "        context:            Context window size \n",
    "        downsampling:       Downsample setting for frequent words\n",
    "        epoch:              Number of Epochs\n",
    "        PrintLogInfo:       Printing Log INFO\n",
    "\n",
    "    @output:\n",
    "        model: trained model over the dataset\n",
    "    '''\n",
    "\n",
    "    # Print log info\n",
    "    if PrintLogInfo:\n",
    "        logging.basicConfig(format='\\f\\r%(asctime)s : %(levelname)s : %(message)s',level=logging.INFO)\n",
    "\n",
    "    # model name (to save model)\n",
    "    MODEL_NAME = \"word2vec_model(500-40-25)\"\n",
    "\n",
    "    # if model exists\n",
    "    if os.path.isfile(MODEL_NAME):\n",
    "        model = word2vec.Word2Vec.load(\"300features_40minwords_10context\")\n",
    "    else:\n",
    "        # otherwise train\n",
    "        print (\"Training model...\")\n",
    "        model = word2vec.Word2Vec(sentences, workers=num_workers, \\\n",
    "                    vector_size=num_features, min_count = min_word_count, \\\n",
    "                    window = context, sample = downsampling,epochs=epoch)\n",
    "\n",
    "        # init_sims will make the model much more memory-efficient.\n",
    "        model.init_sims(replace=True)\n",
    "\n",
    "        # save the model\n",
    "        model.save(MODEL_NAME)\n",
    "    return model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate vectors from reviews "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find sum of occurrence of each word of wordlist in sen \n",
    "def occurrence_of_list(sen,wordlist):\n",
    "    '''\n",
    "    @param;\n",
    "        sen: list of words (a review)\n",
    "        wordlist: list of words (row of a word_matrix)\n",
    "    '''\n",
    "    count = 0\n",
    "    for word in wordlist:\n",
    "        count += operator.countOf(sen,word)\n",
    "    return count\n",
    "\n",
    "# generate word matrix\n",
    "def generate_word_list(model,words,e):\n",
    "    '''\n",
    "    @param:\n",
    "        model: model to determine similar words\n",
    "        words: list of strings (word_list vector)\n",
    "        e: number of columns (max number of similar words to be found)\n",
    "    '''\n",
    "\n",
    "    wordmatrix = [] # empty word matrix\n",
    "    for word in words:\n",
    "        templist = [word] # add the word\n",
    "        try:\n",
    "            # add the top e most similar words  \n",
    "            newlist = np.array(model.wv.most_similar(word,topn=e))[:,0].tolist()\n",
    "        except:\n",
    "            print(\"Not Found: \",word)\n",
    "            newlist = [] # if word is not in vocablary of model\n",
    "        # add row\n",
    "        wordmatrix.append(templist + newlist)\n",
    "    return wordmatrix\n",
    "# generate vectors\n",
    "def generate_vector_indirect(rev_clean,word_matrix):\n",
    "    '''\n",
    "    @param:\n",
    "        rev_clean: list of list of words  (cleaned reviews)\n",
    "        word_matrix: list of list of words (word matrix)\n",
    "\n",
    "    @ouput:\n",
    "        list of list of numbers (list of vectors representing reviews)\n",
    "    '''\n",
    "    print(\"Generating Vectors\")\n",
    "    return [\n",
    "            [\n",
    "                occurrence_of_list(review,word_list)  # find sum of occurrence of each word of word_list in review\n",
    "                for word_list in word_matrix\n",
    "            ] \n",
    "            for review in rev_clean\n",
    "        ]\n",
    "        \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reviews to Vectors"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Labeled Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning Started\n",
      "Cleaned: 25000/25000\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\n",
      "Reviews Cleaned!\n"
     ]
    }
   ],
   "source": [
    "review_unclean = labelled_train['review'].to_numpy() # uncleaned reviews\n",
    "sentiments = labelled_train['sentiment'].to_numpy()  # sentiments\n",
    "\n",
    "# cleaned reviews and flatten words\n",
    "review_clean,all_words = clean_review(review_unclean,True,True,True,True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Producing vectors using respective method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Vectors\n"
     ]
    }
   ],
   "source": [
    "METHOD  = 2 # (Default)\n",
    "feature_size = 1000     # word vector size\n",
    "incr = 2                # skipping intial most frequenct words\n",
    "\n",
    "# generating word vector of size (feature_size) with increment of (incr)\n",
    "words = word_frequency_in_descending_order(all_words)[incr:feature_size+incr]\n",
    "\n",
    "if METHOD == 1:\n",
    "\n",
    "    # generating vectors\n",
    "    review_vectors = generate_vector_direct(review_clean,words)\n",
    "elif METHOD == 2:\n",
    "\n",
    "    # dataset\n",
    "    sentences = prepare_dataset()\n",
    "\n",
    "    #model training\n",
    "    model = train_model(sentences=sentences,num_features=500,min_word_count=40,\n",
    "                        num_workers=4,context=25,downsampling=1e-3,epoch=10)\n",
    "    \n",
    "    #word matrix\n",
    "    words_of_word = generate_word_list(model,words,10)\n",
    "\n",
    "    # generating vectors\n",
    "    review_vectors = generate_vector_indirect(review_clean,words_of_word)\n",
    "print(\"Vector Generation Completed\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_forest_classifier(train_features,train_labels,test_features,test_labels,n_estimators = 1000, seed = 42):\n",
    "    \n",
    "    # Train the model on training data\n",
    "    rf = RandomForestRegressor(n_estimators = n_estimators, random_state = seed)\n",
    "    rf.fit(train_features, train_labels)\n",
    "\n",
    "    # prediction on validation dataset\n",
    "    predictions = rf.predict(test_features)\n",
    "    predictions = [1 if p >= 0.5 else 0 for p in predictions]\n",
    "\n",
    "    # Accuracy\n",
    "    predictions = np.array(predictions)\n",
    "    test_labels = np.array(test_labels)\n",
    "    accuracy = 1 - np.count_nonzero(test_labels-predictions)/len(test_labels)\n",
    "\n",
    "    return rf,accuracy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_classifier(train_features,train_labels,test_features,test_labels,kernel=\"linear\"):\n",
    "    \n",
    "    # Train the model on training data\n",
    "    svm_clf = svm.SVC(kernel=kernel)\n",
    "    svm_clf.fit(train_features,train_labels)\n",
    "\n",
    "    # prediction on validation dataset\n",
    "    predictions = svm_clf.predict(test_features)\n",
    "\n",
    "    # Accuracy\n",
    "    test_labels = np.array(test_labels)\n",
    "    accuracy = 1 - np.count_nonzero(test_labels-predictions)/len(test_labels)\n",
    "\n",
    "    return svm_clf,accuracy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K Nearest Neighours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KNN_classifier(train_features,train_labels,test_features,test_labels,k=47):\n",
    "\n",
    "    # Train the model on training dataset\n",
    "    knn_clf = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn_clf.fit(train_features, train_labels)\n",
    "\n",
    "    # prediction on validation dataset\n",
    "    predictions = knn_clf.predict(test_features)\n",
    "\n",
    "    # Accuracy\n",
    "    test_labels = np.array(test_labels)\n",
    "    accuracy = 1 - np.count_nonzero(test_labels-predictions)/len(test_labels)\n",
    "\n",
    "    return knn_clf,accuracy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_bayes_classifier(train_features,train_labels,test_features,test_labels,distribution=\"LDA\"):\n",
    "\n",
    "    # Train the model on training dataset\n",
    "    if distribution.upper() == \"LDA\":\n",
    "        bayes_clf = LinearDiscriminantAnalysis()\n",
    "    elif distribution.upper() == \"GAUSSIAN\":\n",
    "        bayes_clf = GaussianNB()\n",
    "    elif distribution.upper() == \"QDA\":\n",
    "        bayes_clf = QuadraticDiscriminantAnalysis()\n",
    "    bayes_clf.fit(train_features,train_labels)\n",
    "\n",
    "    # Prediction on validation Dataset\n",
    "    predictions = bayes_clf.predict(test_features)\n",
    "\n",
    "    # Accuracy\n",
    "    test_labels = np.array(test_labels)\n",
    "    accuracy = 1 - np.count_nonzero(test_labels-predictions)/len(test_labels)\n",
    "\n",
    "    return bayes_clf,accuracy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logReg_classifier(train_features,train_labels,test_features,test_labels):\n",
    "\n",
    "    # Train the model on training dataset\n",
    "    log_clf = LogisticRegression()\n",
    "    log_clf.fit(train_features,train_labels)\n",
    "\n",
    "    # Prediction on validation dataset\n",
    "    predictions = log_clf.predict(test_features)\n",
    "\n",
    "    # Accuracy\n",
    "    test_labels = np.array(test_labels)\n",
    "    accuracy = 1 - np.count_nonzero(test_labels-predictions)/len(test_labels)\n",
    "\n",
    "    return log_clf,accuracy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Features Shape: (18750, 1000)\n",
      "Training Labels Shape: (18750,)\n",
      "Testing Features Shape: (6250, 1000)\n",
      "Testing Labels Shape: (6250,)\n"
     ]
    }
   ],
   "source": [
    "# spliting datset into train and valid dataset\n",
    "train_features, valid_features, train_labels, valid_labels = train_test_split(review_vectors,sentiments, test_size = 0.25, random_state = 42)\n",
    "print('Training Features Shape:', np.array(train_features).shape)\n",
    "print('Training Labels Shape:', np.array(train_labels).shape)\n",
    "print('Testing Features Shape:', np.array(valid_features).shape)\n",
    "print('Testing Labels Shape:', np.array(valid_labels).shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier and Their Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression : 0.8544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sagittarius/.local/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "CLASSIFIER = [\"LogisticRegression\",\"SVM\",\"NaiveBayes\",\"KNN\",\"RandomForest\"]\n",
    "num = 0 # choose classifier\n",
    "\n",
    "if CLASSIFIER[num] == \"LogisticRegression\":\n",
    "    model,accuracy = logReg_classifier(train_features,train_labels,valid_features,valid_labels)\n",
    "    print(\"{} : {}\".format(CLASSIFIER[num],accuracy))\n",
    "elif CLASSIFIER[num] == \"SVM\":\n",
    "    model,accuracy = svm_classifier(train_features,train_labels,valid_features,valid_labels)\n",
    "    print(\"{} : {}\".format(CLASSIFIER[num],accuracy))\n",
    "elif CLASSIFIER[num] == \"NaiveBayes\":\n",
    "    model,accuracy = naive_bayes_classifier(train_features,train_labels,valid_features,valid_labels)\n",
    "    print(\"{} : {}\".format(CLASSIFIER[num],accuracy))\n",
    "elif CLASSIFIER[num] == \"KNN\":\n",
    "    model,accuracy = KNN_classifier(train_features,train_labels,valid_features,valid_labels)\n",
    "    print(\"{} : {}\".format(CLASSIFIER[num],accuracy))\n",
    "elif CLASSIFIER[num] == \"RandomForest\":\n",
    "    model,accuracy = random_forest_classifier(train_features,train_labels,valid_features,valid_labels)\n",
    "    print(\"{} : {}\".format(CLASSIFIER[num],accuracy))\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare TestData "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_review_unclean = test['review'].to_numpy()\n",
    "\n",
    "# cleaning test data\n",
    "test_review_clean = clean_review(test_review_unclean,True,True)\n",
    "\n",
    "# vectorizing \n",
    "if METHOD == 1:\n",
    "    test_review_vector = generate_vector_direct(test_review_clean,words)\n",
    "elif METHOD == 2:\n",
    "    test_review_vector = generate_vector_indirect(test_review_clean,words_of_word)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save To Submission.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = test['id'].to_list()\n",
    "test_prediction = model.predict(test_review_vector) # predicition using classification model\n",
    "\n",
    "# creating empty dataFrame\n",
    "test_output_df = pd.DataFrame(columns = [\"id\",\"sentiment\"])\n",
    "test_output_df['id'] = ids # IDs\n",
    "test_output_df['sentiment'] = test_prediction #Prediction\n",
    "\n",
    "# Saving to submission.csv\n",
    "test_output_df.to_csv(\"submission.csv\",index=False,sep=\",\",encoding='utf-8')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
